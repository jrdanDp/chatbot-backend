from langchain_ollama.llms import OllamaLLM  # âœ… cambio de import
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.callbacks import CallbackManager  # âœ… necesario para streaming real
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # âœ… muestra tokens
from app.rag_loader import retriever
import uuid
from typing import Generator

# Diccionarios para mantener el estado por sesiÃ³n
session_memories = {}
session_names = {}
session_greeted = set()

def stream_chatbot(user_input: str, session_id: str = None) -> Generator[str, None, None]:
    if session_id is None:
        session_id = str(uuid.uuid4())

    if session_id not in session_memories:
        session_memories[session_id] = ConversationBufferMemory(
            ai_prefix="Asistente",
            human_prefix="Usuario",
            memory_key="chat_history",
            return_messages=True
        )

    memory = session_memories[session_id]

    if session_id not in session_names:
        if any(word in user_input.lower() for word in ["soy", "me llamo", "mi nombre es"]):
            posibles = user_input.split()
            for i, palabra in enumerate(posibles):
                if palabra.lower() in ["soy", "llamo", "es"] and i + 1 < len(posibles):
                    session_names[session_id] = posibles[i + 1].capitalize()
                    break
        else:
            yield "Hola, Â¿cÃ³mo te llamas? Me gustarÃ­a saber tu nombre para poder acompaÃ±arte mejor."
            return

    if session_id not in session_greeted:
        nombre = session_names.get(session_id, "")
        session_greeted.add(session_id)
        yield f"Â¡Hola {nombre}! Me alegra hablar contigo. Â¿CÃ³mo te sientes hoy?"
        return

    prompt = PromptTemplate.from_template(
        """
    Eres un compaÃ±ero emocional digital que conversa en **espaÃ±ol** como un amigo cercano.

    ðŸ§  Estilo:
    - Responde con frases breves (2 a 5 lÃ­neas como mÃ¡ximo).
    - Usa emojis suaves (â¤ï¸ âœ¨) solo cuando sea natural.
    - MantÃ©n un tono cÃ¡lido, afectuoso y cercano.
    - No repitas tu presentaciÃ³n ni digas que eres una IA.

    ðŸŽ¯ Enfoque de respuestas (proporciÃ³n 70/30):
    - 70% del tiempo: Da consejos simples, recomendaciones prÃ¡cticas o ideas que puedan ayudar.
    - 30% del tiempo: Valida emociones o profundiza con preguntas abiertas.

    ðŸ“Œ Prioriza este flujo:
    1. Valida brevemente la emociÃ³n (si aplica).
    2. Ofrece un consejo o apoyo prÃ¡ctico de forma clara y afectuosa.
    3. Solo si es oportuno, aÃ±ade una pregunta breve para invitar a compartir mÃ¡s.

    Ejemplos:
    - Consejo breve â†’ "PodrÃ­as probar escribir lo que sientes. A veces ayuda â¤ï¸"
    - Sugerencia prÃ¡ctica â†’ "Salir a caminar unos minutos puede ayudarte a despejar la mente."
    - ValidaciÃ³n â†’ "Siento que estÃ©s pasando por esto... Â¿QuÃ© te ayudarÃ­a ahora?"
    - EstÃ­mulo â†’ "Â¡QuÃ© bien! Â¿QuÃ© fue lo que mÃ¡s te gustÃ³? âœ¨"

    Historial de conversaciÃ³n:
    {chat_history}

    InformaciÃ³n recuperada:
    {context}

    Pregunta del usuario:
    {question}
    """
    )



    # âœ… LLM con streaming real usando CallbackManager
    llm = OllamaLLM(
        model="mistral",
        streaming=True,
        callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])
    )

    # Conversational RAG
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=False
    )

    # Asegurar que siempre se yield algo
    try:
        response_stream = qa_chain.stream({"question": user_input})
        for chunk in response_stream:
            content = chunk.get("answer", "")
            if content:
                yield content
    except Exception as e:
        yield f"Lo siento, ocurriÃ³ un error al procesar tu mensaje: {str(e)}"